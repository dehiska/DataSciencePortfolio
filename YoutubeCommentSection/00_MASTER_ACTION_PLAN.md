# Spec A Execution Plan - Master Action Document

## Executive Summary

You want to build a **YouTube Comment Toxicity Detector** following Spec A. Here's your path:

**Immediate (This Week):**
1. âœ… Get YouTube Data API key (see `YOUTUBE_API_SETUP.md`)
2. âœ… Run GCP setup script (`gcp_setup.sh`)
3. âœ… Clone CraigslistWebScraper, adapt structure
4. âœ… Set up local Python 3.10+ environment with PyTorch/HF

**Phase 1 (Week 2-3):**
- Download 8 datasets locally
- Upload to GCP via `gsutil`
- Build local data loader & preprocessing
- Create first Jupyter notebooks (EDA, training)

**Phase 2 (Week 4-5):**
- Train baseline model on 5 core datasets
- Fine-tune on YouTube comments (Phase 1 silver labels)
- Build Streamlit UI for labeling (local)
- Implement MC Dropout + SHAP uncertainty

**Phase 3 (Week 6+):**
- Deploy Cloud Functions (ingestion + scoring)
- Set up Vertex AI Pipeline
- Deploy Streamlit to Cloud Run
- Launch on DigitalOcean

---

## Your Answers to Your 6 Questions

### 1ï¸âƒ£ **Cheapest Way to Start with Datasets + gsutil**

âœ… **Strategy:** Download locally â†’ Upload once to GCP â†’ Keep in cloud storage

```bash
# Step 1: Download all 8 datasets locally (2.5GB)
bash download_datasets.sh

# Step 2: Upload to GCP (one-time cost ~$0.02)
gsutil -m cp -r ~/datasets/youtube-toxicity/* \
  gs://youtube-toxicity-data-v1/raw/datasets/

# Step 3: Reference from GCP in training pipelines
# Cost for storage: ~$0.02/month (negligible with credits)
# Cost for training: FREE (you have VertexAI credits!)
# Cost for inference: ~$0.01-0.05 per 1000 predictions
```

**Total monthly cost with your GCP credits:** ğŸ‰ **$0.00**

**See:** `IMPLEMENTATION_ROADMAP.md` â†’ Part 1-2 for full guide

---

### 2ï¸âƒ£ **YouTube API Key - Updated**

âœ… **Complete step-by-step guide created:** See `YOUTUBE_API_SETUP.md`

**TL;DR:**
1. Create GCP project: `youtubecommentsanalysis-487823` âœ… Done
2. Enable YouTube Data API v3 âœ… Done
3. Create API Key â†’ store in `.env` as `Youtube_Api_key` âœ… Done
4. (Optional) Web app OAuth credential in GCP for future user-auth features

**Auth strategy:** API Key only â€” sufficient for all public YouTube data.
No OAuth flow needed in the ingestion pipeline or Streamlit dashboard.

**Deployment target:** Portfolio website at denissoulimaportfolio.com

**Cost:** ğŸ‰ **FREE** (10,000 quota units/day for free tier)

---

### 3ï¸âƒ£ **CraigslistWebScraper Reference**

âœ… **Your repo:** https://github.com/dehiska/CraigslistWebScraper

**How to use it:**
```bash
# Clone as template
git clone https://github.com/dehiska/CraigslistWebScraper.git
cd CraigslistWebScraper

# Study these files to understand pattern:
# .github/workflows/  â†’ deployment automation
# cloud/functions/    â†’ Cloud Functions structure  
# tests/              â†’ test patterns
# README.md           â†’ deployment walkthrough

# Then create new repo for YouTube project
cd ..
git clone https://github.com/YOUR_USERNAME/YoutubeCommentToxicityDetector.git
# Copy structure and adapt for YouTube
```

**Key patterns to reuse:**
- Cloud Functions + Cloud Scheduler (hourly ingestion)
- GitHub Actions â†’ Workload Identity â†’ GCP deployment
- BigQuery for analytics
- Cloud Storage raw/processed zones

---

### 4ï¸âƒ£ **Adapted Bash Commands for YouTube Project**

âœ… **Complete script created:** See `gcp_setup.sh`

This script is a **direct adaptation** of your Craigslist guide with YouTube-specific names:

```bash
# Run this in Cloud Shell (same pattern as your Craigslist setup)
bash gcp_setup.sh

# What it does:
# âœ… Enable APIs (cloudfunctions, run, scheduler, aiplatform, bigquery)
# âœ… Create service accounts (youtube-runtime, youtube-deployer)
# âœ… Set up Workload Identity Federation (GitHub â†’ GCP)
# âœ… Configure IAM roles (all permissions)
# âœ… Create GCS bucket (youtube-toxicity-data-v1)
# âœ… Configure permissions
# âœ… Output GitHub Actions variables

# Then copy/paste output to GitHub Secrets
```

**See:** `gcp_setup.sh` for full implementation

---

### 5ï¸âƒ£ **Web Scraper Online (GCP), Analysis Local First**

âœ… **Two-environment strategy:**

**LOCAL (Your laptop):**
```
â”œâ”€â”€ Notebooks:
â”‚   â”œâ”€â”€ 01_explore_datasets.ipynb    â† Data exploration
â”‚   â”œâ”€â”€ 02_preprocess_datasets.ipynb â† Preprocessing
â”‚   â”œâ”€â”€ 03_train_baseline_model.ipynb â† Model training
â”‚   â”œâ”€â”€ 04_evaluate_uncertainty.ipynb â† Analysis
â”‚   â””â”€â”€ 05_active_learning_ui.ipynb  â† Streamlit dev
â”œâ”€â”€ Python environment (venv)
â””â”€â”€ datasets/ (downloaded locally)
```

**CLOUD (GCP + portfolio site):**
```
â”œâ”€â”€ Cloud Functions:
â”‚   â”œâ”€â”€ youtube-comments-ingest â†’ pulls YouTube API (API Key auth)
â”‚   â””â”€â”€ youtube-toxicity-scorer â†’ runs model on new comments
â”œâ”€â”€ Cloud Scheduler:
â”‚   â””â”€â”€ Triggers ingest hourly
â”œâ”€â”€ BigQuery:
â”‚   â””â”€â”€ Stores raw comments + predictions + aggregates
â”œâ”€â”€ Vertex AI Pipelines:
â”‚   â””â”€â”€ Automates: preprocess â†’ train â†’ evaluate â†’ deploy
â””â”€â”€ Streamlit on Cloud Run:
    â””â”€â”€ Research dashboard (reads from BigQuery)
    â””â”€â”€ Embedded/linked from denissoulimaportfolio.com
```

**Timeline:**
- Weeks 1-4: **Everything LOCAL** using Jupyter + local Streamlit
- Week 5+: **Deploy ingestion & dashboard** to GCP
- Week 7+: **Move production** to DigitalOcean (if desired)

**See:** `IMPLEMENTATION_ROADMAP.md` â†’ Part 3-4

---

### 6ï¸âƒ£ **Python 3.10+, PyTorch, HuggingFace (Industry Standard)**

âœ… **Full setup in `IMPLEMENTATION_ROADMAP.md` â†’ Part 4**

```bash
# Create environment
python -m venv venv
source venv/bin/activate

# Install stack
pip install -r requirements.txt

# Key packages:
torch==2.0.1              # PyTorch (GPU-optimized)
transformers==4.30.2      # Hugging Face (roberta-base model)
google-cloud-*            # GCP integration
streamlit==1.26.0         # Dashboard
shap==0.42.2              # Interpretability
```

**Why these?**
- âœ… **PyTorch:** Industry standard for DL + better GPU support
- âœ… **HuggingFace:** Pre-trained models + community support
- âœ… **roberta-base:** State-of-art NLP for text classification
- âœ… **PyTorch Lightning:** (optional) simplifies training loops
- âœ… **SHAP:** Explainability + trust for model decisions
- âœ… **Google Cloud:** Integrates w/ your GCP credits

---

## ğŸ¯ **Your Next 3 Steps (TODAY)**

### Step 1: YouTube API Key âœ… DONE
API Key in `.env`, packages installed, `test.py` verified working.

### Step 2: Create GCP Project Setup
```bash
# In Cloud Shell (from Google Cloud Console):
bash gcp_setup.sh

# Copy output to GitHub Secrets
```

### Step 3: Clone & Adapt CraigslistWebScraper
```bash
# Study the structure
git clone https://github.com/dehiska/CraigslistWebScraper.git craigslist-ref

# Create new repo (or in your existing YoutubeCommentSection folder)
# Copy workflows, structure, adapt names

# Key files to copy/adapt:
# - .github/workflows/ YAMLs
# - cloud/functions/ structure
# - requirements.txt patterns
```

---

## ğŸ“š **File Reference**

| File | Purpose | Read First |
|------|---------|------------|
| `SpecA.md` | Full project specification | âœ“ (you created it) |
| `YOUTUBE_API_SETUP.md` | API key instructions | **âœ“ DO THIS FIRST** |
| `gcp_setup.sh` | GCP infrastructure setup | Run after API key |
| `IMPLEMENTATION_ROADMAP.md` | Detailed implementation | Reference during coding |
| This file | Master action plan | **You're reading it!** |

---

## ğŸ’¡ **Why This Approach is Good for Your Goals**

> "The point of this project is to make me become a data engineer and data scientist intern."

### Data Engineer Skills You'll Build:
âœ… **Cloud Infrastructure:** GCP (Cloud Functions, Scheduler, BigQuery, Storage)  
âœ… **CI/CD:** GitHub Actions + Workload Identity  
âœ… **Data Pipelines:** ETL with Vertex AI Pipelines  
âœ… **Scripting:** Bash + Python automation  
âœ… **IaC:** Infrastructure as Code (bash scripts)  

### Data Scientist Skills You'll Build:
âœ… **Model Development:** PyTorch + HuggingFace  
âœ… **Uncertainty Quantification:** MC Dropout + SHAP  
âœ… **Active Learning:** Streamlit labeling UI  
âœ… **Evaluation Metrics:** PR-AUC, F1, Brier Score, ECE  
âœ… **Data Analysis:** Jupyter notebooks  
âœ… **Dashboards:** Streamlit for exploration  

### Both (Industry Standard):
âœ… All tools are **currently used in production** (Meta, Google, etc.)  
âœ… **Resume-friendly:** PyTorch, GCP, Streamlit are hot skills  
âœ… **Hiring signal:** End-to-end ML system (not just notebooks)  

---

## â° **Timeline Estimate**

| Phase | Tasks | Time | Status |
|-------|-------|------|--------|
| **Setup** | API key + GCP + git | 1-2 days | â³ Do now |
| **M1** | Download datasets + local ingestion | 2-3 days | â³ Week 1 |
| **M2** | Train baseline model | 3-5 days | â³ Week 2-3 |
| **M3** | Build Streamlit dashboard (local) | 3 days | â³ Week 3 |
| **M4** | Collect 500-1k gold labels | 5-7 days | â³ Week 4-5 |
| **M5** | Deploy to GCP (functions + scheduler) | 3-5 days | â³ Week 6 |
| **Polish** | Tests + docs + DigitalOcean prep | 3-5 days | â³ Week 7 |

**Total effort:** ~4-6 weeks, part-time  
**Result:** Polished portfolio project + hired as intern! ğŸ‰

---

## â“ **Questions Before You Start?**

Ask me about:
- How to adapt specific workflows from CraigslistWebScraper
- Python environment setup issues
- GCP commands that don't work
- Model architecture choices
- Streamlit UI design
- Deployment strategies

**Let's do this!** ğŸš€

